# ... imports ìƒëµ ...
from app.schema.crawler import DocumentCreate, CrawlerFilter # Filter ì„í¬íŠ¸ ì¶”ê°€

class NationalAssemblyCrawlerService:
    # ... init ìƒëµ ...

    # ----------------------------------------
    # ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ (í•„í„° ì ìš© ë²„ì „)
    # ----------------------------------------
    async def na_crawl(self, filters: CrawlerFilter):
        base_url = settings.NA_BASE_URL 
        list_url = base_url + settings.NA_LIST_URL 
        main_url = base_url + settings.NA_MAIN_URL
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
            "Referer": main_url
        }

        all_data = []
        current_page = 1
        total_collected_count = 0 
        
        # ë‚ ì§œ í¬ë§· ë³€í™˜ (date -> "YYYY.MM.DD")
        sdate_str = filters.start_date.strftime("%Y.%m.%d") if filters.start_date else ""
        edate_str = filters.end_date.strftime("%Y.%m.%d") if filters.end_date else ""

        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘ | ì œí•œ: {filters.limit}ê°œ | ëŒ€ìˆ˜: {filters.parliament_num or 'ì „ì²´'} | ê¸°ê°„: {sdate_str}~{edate_str}")

        async with httpx.AsyncClient(headers=headers, timeout=30.0, follow_redirects=True) as client:
            # [Step 1] ì„¸ì…˜ íšë“
            await client.get(main_url)

            while True:
                # ëª©í‘œ ìˆ˜ëŸ‰ ë„ë‹¬ ì‹œ ì¢…ë£Œ
                if total_collected_count >= filters.limit:
                    print(f"ğŸ›‘ ëª©í‘œ ìˆ˜ëŸ‰({filters.limit}ê°œ) ë‹¬ì„±.")
                    break

                print(f"\nğŸ“„ [Page {current_page}] ìš”ì²­ ì¤‘...")

                # [Step 2] íŒŒë¼ë¯¸í„° êµ¬ì„± (ì„œë²„ í•„í„°ë§)
                params = {
                    "page": current_page,
                    "limit": 10,
                    "sdate": sdate_str,  # ì‹œì‘ì¼ (ì„œë²„ í•„í„°)
                    "edate": edate_str,  # ì¢…ë£Œì¼ (ì„œë²„ í•„í„°)
                    "flag": "all",       # ì „ì²´ ê²€ìƒ‰
                    # "schword": "ê²€ìƒ‰ì–´" # í•„ìš”ì‹œ ê²€ìƒ‰ì–´ë„ ì¶”ê°€ ê°€ëŠ¥
                }

                res = await client.get(list_url, params=params)
                soup = BeautifulSoup(res.text, "html.parser")
                tbody = soup.select_one("#listData")

                if not tbody: break
                rows = tbody.select("tr")
                
                # ë°ì´í„° ì—†ìŒ ì²´í¬ (êµ­íšŒ ì‚¬ì´íŠ¸ íŠ¹ì„±ìƒ ë¬¸êµ¬ê°€ ëœ° ìˆ˜ ìˆìŒ)
                if not rows or (len(rows) == 1 and "ë°ì´í„°ê°€" in rows[0].text):
                    print("âœ… ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break

                # --- í–‰ ë°˜ë³µ ---
                for row in rows:
                    if total_collected_count >= filters.limit: break

                    cols = row.select("td")
                    if len(cols) < 6: continue

                    try:
                        # 1. ì •ë³´ íŒŒì‹±
                        parliament_text = cols[1].text.strip() # ì˜ˆ: ì œ22ëŒ€êµ­íšŒ
                        date_str = cols[5].text.strip()        # ì˜ˆ: 2024.11.04.
                        
                        # =================================================
                        # ğŸ›¡ï¸ [íŒŒì´ì¬ í•„í„°ë§] ì„œë²„ê°€ ëª» ê±°ë¥¸ ì¡°ê±´ ì²´í¬
                        # =================================================
                        
                        # (1) ëŒ€ìˆ˜ í•„í„° (ì˜ˆ: '22' ì…ë ¥ ì‹œ 'ì œ22ëŒ€' í¬í•¨ ì—¬ë¶€ í™•ì¸)
                        if filters.parliament_num:
                            # "22" ê°€ "ì œ22ëŒ€êµ­íšŒ" ì•ˆì— ì—†ìœ¼ë©´ ìŠ¤í‚µ
                            if filters.parliament_num not in parliament_text:
                                # print(f"   â­ï¸ [Skip] ëŒ€ìˆ˜ ë¶ˆì¼ì¹˜: {parliament_text}")
                                continue

                        # (2) ë‚ ì§œ 2ì°¨ ê²€ì¦ (í˜¹ì‹œ ëª¨ë¥´ë‹ˆ íŒŒì´ì¬ì—ì„œ í™•ì‹¤í•˜ê²Œ)
                        # date_str -> date ê°ì²´ ë³€í™˜
                        try:
                            row_date = datetime.strptime(date_str.rstrip('.'), "%Y.%m.%d").date()
                            
                            if filters.start_date and row_date < filters.start_date: continue
                            if filters.end_date and row_date > filters.end_date: continue
                        except:
                            pass # ë‚ ì§œ íŒŒì‹± ì—ëŸ¬ë‚˜ë©´ ì¼ë‹¨ ì§„í–‰ (or ìŠ¤í‚µ)

                        # -------------------------------------------------
                        # ì´ ì•„ë˜ëŠ” ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼
                        # -------------------------------------------------
                        subject_td = cols[4]
                        a_tag = subject_td.select_one("a")
                        if not a_tag: continue 

                        title = a_tag.text.strip()
                        href = a_tag.get('href')
                        
                        # ... (ID ì¶”ì¶œ, ë‹¤ìš´ë¡œë“œ, DB ì €ì¥ ë¡œì§ì€ ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ë³µë¶™) ...
                        # ... ì¤‘ëµ ...
                        # file_id ì¶”ì¶œ ...
                        # is_crawled ì²´í¬ ...
                        # ë‹¤ìš´ë¡œë“œ ...
                        # save_document ...
                        
                        # ì„±ê³µ ì‹œ ì¹´ìš´íŠ¸ ì¦ê°€
                        # total_collected_count += 1

                    except Exception as e:
                        print(f"   âš ï¸ ì—ëŸ¬: {e}")
                        continue
                
                # í˜ì´ì§€ ì¦ê°€
                current_page += 1
                await asyncio.sleep(1)

        return all_data