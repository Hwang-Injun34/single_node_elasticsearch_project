import os
import re
import random
import asyncio
import httpx
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, parse_qs

from app.repositories.crawler import NationalAssemblyCrawlerRepository
from app.core.config import settings
from app.schema.crawler import DocumentCreate, CrawlerFilter

# PDF ì €ì¥ ê²½ë¡œ ìƒì„±
os.makedirs(settings.PDF_DIR, exist_ok=True)

class NationalAssemblyCrawlerService:
    def __init__(self, repo: NationalAssemblyCrawlerRepository):
        self.db_repo = repo

    # ----------------------------------------
    # ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ (ìœ„ì›íšŒ íšŒì˜ë¡ - JSON API ë°©ì‹)
    # ----------------------------------------
    async def na_crawl(self, filters: CrawlerFilter):
        # 1. URL ì„¤ì • (www.assembly.go.kr ê¸°ì¤€)
        base_domain = settings.NA_BASE_URL # https://www.assembly.go.kr
        
        # CSRF í† í°ì„ ì–»ê¸° ìœ„í•œ ë©”ì¸ í™”ë©´
        main_url = f"{base_domain}/portal/main/contents.do?menuNo=600045"
        
        # ì‹¤ì œ ë°ì´í„°ë¥¼ ìš”ì²­í•  API ì£¼ì†Œ
        api_url = f"{base_domain}/portal/cnts/cntsCmmit/listMtgRcord.json"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
            "Referer": main_url,
            "Origin": base_domain
        }

        all_data = []
        current_page = 1
        total_collected_count = 0 

        # ë‚ ì§œ í•„í„° í¬ë§·íŒ… (YYYYMMDD)
        sdate_str = filters.start_date.strftime("%Y%m%d") if filters.start_date else ""
        edate_str = filters.end_date.strftime("%Y%m%d") if filters.end_date else ""

        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘ | Limit: {filters.limit} | ê¸°ê°„: {sdate_str}~{edate_str}")

        async with httpx.AsyncClient(headers=headers, timeout=30.0, follow_redirects=True) as client:
            
            # =========================================================
            # [Step 1] ë©”ì¸ í˜ì´ì§€ ì ‘ì†í•˜ì—¬ CSRF í† í° íšë“ (í•„ìˆ˜ â­)
            # =========================================================
            print("ğŸšª ë©”ì¸ í˜ì´ì§€ ì ‘ì† ë° ë³´ì•ˆ í† í° í™•ë³´ ì¤‘...")
            try:
                main_res = await client.get(main_url)
                soup = BeautifulSoup(main_res.text, "html.parser")
                
                # <meta name="_csrf" content="..."> íƒœê·¸ ì°¾ê¸°
                csrf_meta = soup.select_one("meta[name='_csrf']")
                
                if not csrf_meta:
                    print("âŒ CSRF í† í°ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ì´íŠ¸ êµ¬ì¡° ë³€ê²½ë¨)")
                    return []
                
                csrf_token = csrf_meta['content']
                # print(f"   ğŸ”‘ Token í™•ë³´ ì™„ë£Œ: {csrf_token[:10]}...")

            except Exception as e:
                print(f"âŒ ì´ˆê¸° ì ‘ì† ì‹¤íŒ¨: {e}")
                return []

            # =========================================================
            # [Step 2] ë°ì´í„° ìš”ì²­ ë£¨í”„ (Pagination)
            # =========================================================
            while True:
                # ëª©í‘œ ìˆ˜ëŸ‰ ë„ë‹¬ ì²´í¬ (-1ì€ ë¬´ì œí•œ)
                if filters.limit != -1 and total_collected_count >= filters.limit:
                    print(f"ğŸ›‘ ëª©í‘œ ìˆ˜ëŸ‰({filters.limit}ê°œ) ë‹¬ì„±ìœ¼ë¡œ ì¢…ë£Œ.")
                    break

                print(f"\nğŸ“„ [Page {current_page}] API ìš”ì²­ ì¤‘...")

                # POST ë°ì´í„° êµ¬ì„± (Form Data)
                payload = {
                    "menuNo": "600045",          # ë©”ë‰´ë²ˆí˜¸ (ê³ ì •)
                    "pageIndex": str(current_page),
                    "cntsDivCd": "CMMIT",        # ì½˜í…ì¸  êµ¬ë¶„ (ìœ„ì›íšŒ)
                    "committeeCd": "",           # ì „ì²´ ìœ„ì›íšŒ
                    "title": "",                 # ê²€ìƒ‰ì–´
                    "beginDate": sdate_str,      # ì‹œì‘ì¼
                    "endDate": edate_str,        # ì¢…ë£Œì¼
                    "_csrf": csrf_token          # â­ í† í° í•„ìˆ˜
                }

                try:
                    # API í˜¸ì¶œ (POST)
                    res = await client.post(api_url, data=payload)
                    data = res.json()
                    
                    # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
                    result_list = data.get("resultList", [])
                    
                    # ì¢…ë£Œ ì¡°ê±´: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë
                    if not result_list:
                        print("âœ… ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    print(f"   ğŸ” {len(result_list)}ê°œì˜ ë¬¸ì„œ ë°œê²¬")

                    # --- ë¦¬ìŠ¤íŠ¸ ë°˜ë³µ ì²˜ë¦¬ ---
                    for item in result_list:
                        # ëª©í‘œ ìˆ˜ëŸ‰ ì²´í¬
                        if filters.limit != -1 and total_collected_count >= filters.limit: break

                        try:
                            # 1. JSON ë°ì´í„° íŒŒì‹±
                            # item ì˜ˆì‹œ: {'committeeName': 'ë²•ì œì‚¬ë²•ìœ„', 'title': '...', 'pdfLinkUrl': '...', ...}
                            
                            title = item.get("title", "").strip()
                            committee_name = item.get("committeeName", "").strip()
                            conf_date_str = item.get("confDate", "").strip() # YYYY.MM.DD
                            pdf_link = item.get("pdfLinkUrl") # PDF ë§í¬ (ì—†ì„ ìˆ˜ ìˆìŒ)

                            # PDF ì—†ëŠ” íšŒì˜ë¡ì€ ìŠ¤í‚µ
                            if not pdf_link:
                                # print(f"   â­ï¸ [Skip] PDF ì—†ìŒ: {title}")
                                continue

                            # 2. íŒŒì¼ ID ì¶”ì¶œ
                            # ë§í¬ ì˜ˆì‹œ: /portal/comm/download/downloadFile.do?fileId=2023...
                            file_id = None
                            if "fileId=" in pdf_link:
                                file_id = pdf_link.split("fileId=")[1].split("&")[0]
                            
                            # IDê°€ ì—†ìœ¼ë©´ ì œëª©+ë‚ ì§œë¡œ ê³ ìœ  ID ìƒì„± (ì¤‘ë³µ ì²´í¬ìš©)
                            if not file_id:
                                safe_temp_title = re.sub(r'\s+', '', title)
                                file_id = f"{conf_date_str}_{safe_temp_title}"

                            # 3. ì¤‘ë³µ ì²´í¬ (DB)
                            if await self.db_repo.is_crawled(doc_id=file_id):
                                print(f"   â­ï¸ [Skip] ì´ë¯¸ ìˆ˜ì§‘ë¨: {title}")
                                continue

                            print(f"   â–¶ï¸ ìˆ˜ì§‘ ì‹œì‘: {title}")

                            # 4. ë‹¤ìš´ë¡œë“œ URL ì™„ì„±
                            if not pdf_link.startswith("http"):
                                download_url = base_domain + pdf_link
                            else:
                                download_url = pdf_link

                            # 5. íŒŒì¼ëª… ìƒì„± ë° ê²½ë¡œ ì„¤ì •
                            safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:50]
                            filename = f"{conf_date_str}_{file_id}_{safe_title}.pdf"
                            file_path = os.path.join(settings.PDF_DIR, filename)

                            # 6. íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                            file_res = await client.get(download_url)
                            if file_res.status_code == 200:
                                with open(file_path, "wb") as f:
                                    f.write(file_res.content)
                                

                                # 8. DB ì €ì¥ (Schema -> DB Model)
                                doc_data = DocumentCreate(
                                    doc_id=file_id,
                                    title=safe_title,
                                    committee_name=committee_name,
                                    meeting_date=meeting_date,
                                    file_url=download_url,
                                    file_path=file_path,
                                    # ìœ„ì›íšŒ íšŒì˜ë¡ì—” ëŒ€/íšŒê¸°/ì°¨ ì •ë³´ê°€ JSONì— ì—†ì„ ìˆ˜ ìˆìŒ (ë¹ˆê°’ ì²˜ë¦¬)
                                    dae_num="",
                                    session_num="",
                                    degree_num=""
                                )

                                # model_dump()ë¡œ ë”•ì…”ë„ˆë¦¬ ë³€í™˜ í›„ ì „ë‹¬
                                await self.db_repo.save_document(doc_data.model_dump())
                                
                                all_data.append({"id": file_id, "status": "success"})
                                total_collected_count += 1
                                
                                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                                await asyncio.sleep(random.uniform(0.5, 1.2))
                            else:
                                print(f"      âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({file_res.status_code})")

                        except Exception as e:
                            print(f"      âš ï¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                            continue
                    
                    # --- í˜ì´ì§€ ì¦ê°€ ---
                    current_page += 1
                    await asyncio.sleep(1) # í˜ì´ì§€ ë„˜ê¹€ ëŒ€ê¸°

                except Exception as e:
                    print(f"   âš ï¸ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {e}")
                    break

        print(f"ğŸ í¬ë¡¤ë§ ì¢…ë£Œ! ì´ {total_collected_count}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
        return all_data